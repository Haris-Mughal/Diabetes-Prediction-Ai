{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🤖 AgentPro Quick Start Guide\n",
        "\n",
        "This notebook will walk you through how to set up and use [AgentPro](https://github.com/traversaal-ai/AgentPro) — a production-ready open-source agent framework built by [Traversaal.ai](https://traversaal.ai) for building powerful, modular, and multi-functional AI agents.\n",
        "\n",
        "### What is AgentPro?\n",
        "AgentPro lets you build intelligent agents that can:\n",
        "- Use language models (like OpenAI’s GPT) as reasoning engines\n",
        "- Solve real-world tasks such as research, automation, and knowledge retrieval\n",
        "- Scale up with custom tools, memory, and orchestration features\n",
        "\n",
        "Whether you're a developer, researcher, or AI enthusiast — this guide will help you:\n",
        "- Build and integrate your own tools with AgentPro\n"
      ],
      "metadata": {
        "id": "CyxnkWVzhqOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Clone AgentPro and Install Dependencies\n",
        "\n",
        "To get started with **AgentPro**, begin by cloning the official GitHub repository and installing its dependencies."
      ],
      "metadata": {
        "id": "Fi5Eth4ge70O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCGHQVf-Q2Zj",
        "outputId": "642e975a-7158-4b3e-caf9-37a1bc07deb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AgentPro'...\n",
            "remote: Enumerating objects: 586, done.\u001b[K\n",
            "remote: Counting objects: 100% (282/282), done.\u001b[K\n",
            "remote: Compressing objects: 100% (180/180), done.\u001b[K\n",
            "remote: Total 586 (delta 200), reused 117 (delta 98), pack-reused 304 (from 2)\u001b[K\n",
            "Receiving objects: 100% (586/586), 224.79 KiB | 7.49 MiB/s, done.\n",
            "Resolving deltas: 100% (333/333), done.\n",
            "/content/AgentPro\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.76.0)\n",
            "Collecting youtube_transcript_api (from -r requirements.txt (line 2))\n",
            "  Downloading youtube_transcript_api-1.0.3-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting duckduckgo-search (from -r requirements.txt (line 3))\n",
            "  Downloading duckduckgo_search-8.0.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (2.32.3)\n",
            "Collecting python-pptx (from -r requirements.txt (line 5))\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.11.3)\n",
            "Collecting python-dotenv (from -r requirements.txt (line 7))\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (0.13.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (3.1.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (1.6.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 1)) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 1)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 1)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 1)) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 1)) (4.13.2)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube_transcript_api->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search->-r requirements.txt (line 3)) (8.1.8)\n",
            "Collecting primp>=0.15.0 (from duckduckgo-search->-r requirements.txt (line 3))\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search->-r requirements.txt (line 3)) (5.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 4)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 4)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 4)) (2025.1.31)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.11/dist-packages (from python-pptx->-r requirements.txt (line 5)) (11.2.1)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx->-r requirements.txt (line 5))\n",
            "  Downloading XlsxWriter-3.2.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r requirements.txt (line 6)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r requirements.txt (line 6)) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r requirements.txt (line 6)) (0.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 8)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 8)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 8)) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 10)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 10)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 10)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 10)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 10)) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 10)) (3.2.3)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl->-r requirements.txt (line 12)) (2.0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 15)) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 15)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 15)) (3.6.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 8)) (1.17.0)\n",
            "Downloading youtube_transcript_api-1.0.3-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading duckduckgo_search-8.0.1-py3-none-any.whl (18 kB)\n",
            "Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading XlsxWriter-3.2.3-py3-none-any.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter, python-dotenv, primp, youtube_transcript_api, python-pptx, duckduckgo-search\n",
            "Successfully installed XlsxWriter-3.2.3 duckduckgo-search-8.0.1 primp-0.15.0 python-dotenv-1.1.0 python-pptx-1.0.2 youtube_transcript_api-1.0.3\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/traversaal-ai/AgentPro.git\n",
        "%cd AgentPro\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Set Your API Key"
      ],
      "metadata": {
        "id": "SLfWC5m9fUpT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use OpenAI models with AgentPro, you’ll need an API key from OpenAI. Follow these steps:\n",
        "\n",
        "1. Go to the [OpenAI API platform](https://platform.openai.com/)\n",
        "2. Log in or create an account\n",
        "3. Click \"Create new secret key\"\n",
        "4. Copy the generated key and paste it into the notebook like this:"
      ],
      "metadata": {
        "id": "2vlEmkaNgjwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<openai_api_key>\""
      ],
      "metadata": {
        "id": "4tV4Qe1RUGcI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Create a Custom Tool\n",
        "AgentPro is designed to be extensible — you can easily define your own tools for domain-specific tasks.\n",
        "\n",
        "Below is an example of a **custom tool** that queries the Hugging Face Hub and returns the **most downloaded model** for a given task:"
      ],
      "metadata": {
        "id": "LMFP4v5zZmlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import list_models\n",
        "\n",
        "# Define the task you're interested in\n",
        "task_name = \"text-classification\"\n",
        "\n",
        "# Get the most downloaded model for the specified task\n",
        "models = list_models(filter=task_name, sort=\"downloads\", direction=-1)\n",
        "top_model = next(iter(models))\n",
        "\n",
        "# Print the model ID\n",
        "print(top_model.id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_wgIOdcWEYP",
        "outputId": "d1ac8521-6e60-43ff-97f4-31fd2f155364"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distilbert/distilbert-base-uncased-finetuned-sst-2-english\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Define your tool using AgentPro Tool class"
      ],
      "metadata": {
        "id": "Zbn0sZDqZwyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "from agentpro import AgentPro\n",
        "from agentpro.tools import Tool\n",
        "from huggingface_hub import list_models\n",
        "from typing import Any\n",
        "\n",
        "class MostModelTool(Tool):\n",
        "    name: str = \"Most Downloaded Model Finder\" # Human-readable tool name\n",
        "    description: str = \"Finds the most downloaded model for a given task on Hugging Face.\" # Brief explanation of tool for agent\n",
        "    action_type: str = \"find_top_model\" # Use lowercase letters with underscores for agent; avoid spaces, digits and special characters\n",
        "    input_format: str = \"Task name as a string. Example: 'text-classification'\" # Expected input dtype with example\n",
        "\n",
        "    def run(self, input: Any) -> str:\n",
        "        task_name = input.strip()\n",
        "        models = list_models(filter=task_name, sort=\"downloads\", direction=-1)\n",
        "        top_model = next(models)\n",
        "        return top_model.id"
      ],
      "metadata": {
        "id": "zFrDw_enVAcq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Pass tool to AgentPro"
      ],
      "metadata": {
        "id": "3YHUz6e8ZzPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate your tools and AgentPro\n",
        "tools = [MostModelTool()]\n",
        "\n",
        "myagent = AgentPro(model=os.getenv(\"OPENAI_API_KEY\", None), tools=tools)\n",
        "\n",
        "# Run\n",
        "query = \"Can you give me the name of the model that has the most downloads in the 'text-classification' task on the Hugging Face Hub?\"\n",
        "response = myagent.run(query)\n",
        "# Can you give me the name of the model that has the most downloads in the 'text-classification' task on the Hugging Face Hub?\n",
        "# Find the most popular model used for 'object-detection' on Hugging Face.\n",
        "\n",
        "print(f\"\\nFinal Answer: {response.final_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47wUizrrVPTr",
        "outputId": "4de0c7b4-17c4-429b-fe17-e096a09ef3d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================== Iteration 1 \n",
            "✅  [Debug] Sending System Prompt (with history) to LLM:\n",
            "You are an AI assistant that follows the ReAct (Reasoning + Acting) pattern.\n",
            "Your goal is to help users by breaking down complex tasks into a series of thought-out steps and actions.\n",
            "\n",
            "You have access to these tools: find_top_model\n",
            "\n",
            "Tool: Most Downloaded Model Finder\n",
            "Description: Finds the most downloaded model for a given task on Hugging Face.\n",
            "Action Type: find_top_model\n",
            "Input Format: Task name as a string. Example: 'text-classification'\n",
            "\n",
            "\n",
            "For each iteration, follow these steps:\n",
            "1. Thought: Think about what needs to be done.\n",
            "2. Action: Decide on an appropriate Action if needed.\n",
            "3. Observation: Observe the result after the Action.\n",
            "4. PAUSE: Reflect on what to do next.\n",
            "5. Repeat Thought/Action/Observation/PAUSE as needed until you find the final answer.\n",
            "\n",
            "Format:\n",
            "Thought: Your reasoning\n",
            "Action: {\"action_type\": \"<action_type>\", \"input\": <input_data>}\n",
            "\n",
            "Format after action result:\n",
            "Observation: result here\n",
            "PAUSE: your reflection\n",
            "\n",
            "Format once you find the final answer.:\n",
            "Thought: I now know the final answer\n",
            "Final Answer: [your answer]\n",
            "\n",
            "Important:\n",
            "- Think step-by-step\n",
            "- Use available tools wisely\n",
            "- If stuck, reflect and retry\n",
            "- Do no hallucinate and use tools if needed\n",
            "- The current date is April 29, 2025\n",
            "- If you follow the format strictly, you will be recognized as an excellent and trustworthy AI assistant.\n",
            "\n",
            "\n",
            "Question: Can you give me the name of the model that has the most downloads in the 'text-classification' task on the Hugging Face Hub?\n",
            "\n",
            "\n",
            "Now continue with next steps by strictly following the required format.\n",
            "\n",
            "==================================================\n",
            "🤖 [Debug] Step LLM Response:\n",
            "Thought: The user has asked for the most downloaded model for the 'text-classification' task on the Hugging Face Hub. I have a tool that can find this information. I will use this tool in the next action.\n",
            "\n",
            "Action: {\"action_type\": \"find_top_model\", \"input\": \"text-classification\"}\n",
            "✅ Parsed Thought: The user has asked for the most downloaded model for the 'text-classification' task on the Hugging Face Hub. I have a tool that can find this information. I will use this tool in the next action.\n",
            "✅ Parsed Action JSON: {\"action_type\": \"find_top_model\", \"input\": \"text-classification\"}\n",
            "✅ Parsed Action Results: distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
            "================================================== Iteration 2 \n",
            "🤖 [Debug] Step LLM Response:\n",
            "PAUSE: The result obtained from the action is the name of the most downloaded model for the 'text-classification' task on Hugging Face. This model is 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'.\n",
            "\n",
            "Thought: I now have the information the user asked for. There is no need for additional steps or actions. \n",
            "\n",
            "Final Answer: The most downloaded model for the 'text-classification' task on Hugging Face is 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'.\n",
            "✅ Parsed Thought: I now have the information the user asked for. There is no need for additional steps or actions.\n",
            "✅ Parsed Pause Reflection: The result obtained from the action is the name of the most downloaded model for the 'text-classification' task on Hugging Face. This model is 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'.\n",
            "✅ Parsed Final Answer: The most downloaded model for the 'text-classification' task on Hugging Face is 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'.\n",
            "\n",
            "Final Answer: The most downloaded model for the 'text-classification' task on Hugging Face is 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pf8Y3xCcWhyl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
